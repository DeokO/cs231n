{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import graphviz\n",
    "\n",
    "print(\"tensorflow version %s\" % tf.__version__)\n",
    "print(\"numpy      version %s\" % np.__version__)\n",
    "print(\"matplotlib version %s\" % mpl.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Tensorflow Detials\n",
    "Within tensorflow, a `tf.Graph` is a directed acyclic graph of operations (`tf.Operations`) - also known as a call graph. There is a default graph (`tf.get_default_graph()`) and any new operations are added to this graph. `tf.Operations` operate on a `tf.Tensor` and return a `tf.Tensor` ($f:X->Y$, where $X$ and $Y$ are tensors).\n",
    "\n",
    "Evaluation of the graph requires a session (`tf.Session()`). One can construct a global session (or can use context managers so that the sessions are independent - this is useful if wanting to compute something on a separate device). In addition there is a special session `tf.InteractiveSession()` - designed for interactive python environments, and it won't require referencing the specific session."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Using a context manager\n",
    "with tf.Session() as session:\n",
    "    n_values = 32\n",
    "    x = tf.linspace(-3., 3., 32)\n",
    "    \n",
    "    y = x.eval(session=session)\n",
    "    z = session.run(x)\n",
    "    print(\"x (and x[0]) type information:\")\n",
    "    print(\"------------------------------\")\n",
    "    print(\"type(x), x: \",type(x), x)\n",
    "    print(\"type(x[0]), x[0]: \",type(x[0]), x[0])\n",
    "    #type is purely a tensorflow object\n",
    "    # ** It DOESN'T HAVE a Value **\n",
    "    \n",
    "    print()\n",
    "    print(\"evaluated type information (explicit call to session.run or tf.eval):\")\n",
    "    print(\"---------------------------\")\n",
    "    print(type(y), y)\n",
    "    #type is now a numpy object\n",
    "    print(type(z), z)\n",
    "    #type is now a numpy object\n",
    "    \n",
    "    print(type(session.run(x[0])),session.run(x[0]))\n",
    "    #evaluates down to numpy.float32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Can't use an interactive session within a context;\n",
    "try:\n",
    "    with tf.InteractiveSession() as session:\n",
    "        print(\"Inside context\")\n",
    "except AttributeError as e:\n",
    "    print(\"InteractiveSession doesn't have an %s\" % e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Let's try running with the session (defined within the context manager)\n",
    "try:\n",
    "    session.run(x)\n",
    "except RuntimeError as re:\n",
    "    print(\"RuntimeError: %s\" % re)\n",
    "# (session is closed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# tensorflow constants, variables (and lazy evaluation)\n",
    "As mentioned earlier tensorflow stores computation as a call graph - or `dataflow graph` - and will not perform any calculations until the input data state is correct; - this is also known as **lazy evaluation**. There are a few advantages to this:\n",
    "* tensorflow can be run on multiple computation devices (`/device:CPU:0` or `/cpu0`, `/device:GPU:i` or `/gpu:i` (`i`-th GPU device).\n",
    "* lazy evaluation allows for arbitrary depth expression (recursive form);\n",
    "* results that are never used, will never be evaluated\n",
    "\n",
    "This contrasts the default `python` (and `numpy`) behaviour;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# python behaviour:\n",
    "print(\"python behaviour:\")\n",
    "print(\"-----------------\")\n",
    "x = 1\n",
    "y = x + 1\n",
    "print(y)\n",
    "\n",
    "#numpy behaviour\n",
    "print()\n",
    "print(\"numpy behaviour:\")\n",
    "print(\"----------------\")\n",
    "x = np.linspace(-3., 3., 32)\n",
    "y = x + 1\n",
    "print(type(y),y)\n",
    "\n",
    "print()\n",
    "print(\"tensorflow behaviour:\")\n",
    "print(\"---------------------\")\n",
    "with tf.Session() as session:\n",
    "    n_values = 32\n",
    "    x = tf.linspace(-3., 3., 32, name='x')\n",
    "    y = tf.Variable(x + 5, name='y')\n",
    "    print(type(y), y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explicit Initialization and Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with tf.Session() as session:\n",
    "    n_values = 32\n",
    "    x = tf.linspace(-3., 3., 32, name='x')\n",
    "    y = tf.Variable(x + 5, name='y')\n",
    "    init_op = tf.initialize_all_variables()\n",
    "    session.run(init_op)\n",
    "    #initialize the 'variables'\n",
    "    print(session.run(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with tf.Session() as session:\n",
    "    n_values = 32\n",
    "    x = tf.linspace(-3., 3., 32, name='x')\n",
    "    y = tf.Variable(x + 5, name='y')\n",
    "    init_op = tf.initialize_all_variables()\n",
    "    print(\"is y initialized: \", session.run(tf.is_variable_initialized(y)))\n",
    "    #variable is not initialized to explicitly initialized\n",
    "    # (init_op has to be run)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with tf.Session() as session:\n",
    "    n_values = 32\n",
    "    x = tf.linspace(-3., 3., 32, name='x')\n",
    "    y = tf.Variable(x + 5, name='y')\n",
    "    print(\"is y initialized: \", session.run(tf.is_variable_initialized(y)))\n",
    "    session.run(y.initializer)\n",
    "    print(\"is y initialized: \", session.run(tf.is_variable_initialized(y)))\n",
    "    #explicitly initialze 'y'\n",
    "    print(session.run(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with tf.Session() as session:\n",
    "    n_values = 32\n",
    "    x = tf.linspace(-3., 3., 32, name='x')\n",
    "    y = tf.Variable(x + 5, name='y')\n",
    "    z = tf.Variable(y - 3, name='z')\n",
    "    print(\"is y initialized: \", session.run(tf.is_variable_initialized(y)))\n",
    "    session.run(y.initializer)\n",
    "    print(\"is y initialized: \", session.run(tf.is_variable_initialized(y)))\n",
    "    #explicitly initialze 'y'\n",
    "    print(\"is z initialized: \", session.run(tf.is_variable_initialized(z)))\n",
    "    try:\n",
    "        session.run(z)\n",
    "    except tf.errors.FailedPreconditionError as FPE:\n",
    "        print(\"z is uninitialized (as expected)\\n %s\" % FPE)\n",
    "    #z is no initialized (only y is initialized)\n",
    "    init_yz = tf.initialize_variables([y,z])\n",
    "    #initialize variables in the list (and only those variables)\n",
    "    session.run(init_yz)\n",
    "    print(\"is z initialized: \", session.run(tf.is_variable_initialized(z)))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Methods of Initializing Variables:\n",
    " * `tf.Variable.initializer`\n",
    " * `tf.initialize_variable([<list>])`\n",
    " * `tf.initialize_all_variables()`\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Getting a list of all variables:\n",
    "variables = tf.all_variables()\n",
    "for var in variables:\n",
    "    print(var.name)\n",
    "\n",
    "#Notice the names:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Variables:\n",
    "Each time a variable is declared: `tf.Variable(..., name='y')` it is given a unique identifier (unless the variables are declared within a `variable_scope` with `reuse` set to true)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#without shape information get_variable will fail;\n",
    "try:\n",
    "    tf.get_variable(\"y\")\n",
    "except ValueError as ve:\n",
    "    print(\"Require shape information!\\n %s\" % ve)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Variable Scope\n",
    "Imagine you create a simple model for image filters (in this case only 2 convolutions). If you use `tf.Variable` you could define the following functions:\n",
    "```\n",
    "def image_filters(in_image):\n",
    "    conv1_weights = tf.Variable(tf.random_normal([5, 5, 32, 32]),\n",
    "        name=\"conv1_weights\")\n",
    "    conv1_biases = tf.Variable(tf.zeros([32]), name=\"conv1_biases\")\n",
    "    conv1 = tf.nn.conv2d(in_image, conv1_weights,\n",
    "        strides=[1, 1, 1, 1], padding='SAME')\n",
    "    relu1 = tf.nn.relu(conv1 + conv1_biases)\n",
    "\n",
    "    conv2_weights = tf.Variable(tf.random_normal([5, 5, 32, 32]),\n",
    "        name=\"conv2_weights\")\n",
    "    conv2_biases = tf.Variable(tf.zeros([32]), name=\"conv2_biases\")\n",
    "    conv2 = tf.nn.conv2d(relu1, conv2_weights,\n",
    "        strides=[1, 1, 1, 1], padding='SAME')\n",
    "    return tf.nn.relu(conv2 + conv2_biases)\n",
    "```\n",
    "\n",
    "This creates 4 variables per function call; `conv1_biases`, `conv2_biases`, `conv1_weights`, `conv2_weights`;\n",
    "\n",
    "```\n",
    "result1 = image_filter(image_1)\n",
    "result2 = image_filter(image_2)\n",
    "```\n",
    "This would create two sets of variables;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "variables = tf.all_variables()\n",
    "for var in variables:\n",
    "    print(var.name)\n",
    "#cleared the state;\n",
    "\n",
    "def image_filters(in_image):\n",
    "    conv1_weights = tf.Variable(tf.random_normal([3, 3, 1, 32]),\n",
    "        name=\"conv1_weights\")\n",
    "    conv1_biases = tf.Variable(tf.zeros([32]), name=\"conv1_biases\")\n",
    "    conv1 = tf.nn.conv2d(in_image, conv1_weights,\n",
    "        strides=[1, 1, 1, 1], padding='SAME')\n",
    "    relu1 = tf.nn.relu(conv1 + conv1_biases)\n",
    "\n",
    "    conv2_weights = tf.Variable(tf.random_normal([3, 3, 32, 32]),\n",
    "        name=\"conv2_weights\")\n",
    "    conv2_biases = tf.Variable(tf.zeros([32]), name=\"conv2_biases\")\n",
    "    conv2 = tf.nn.conv2d(relu1, conv2_weights,\n",
    "        strides=[1, 1, 1, 1], padding='SAME')\n",
    "    return tf.nn.relu(conv2 + conv2_biases)\n",
    "\n",
    "x = tf.placeholder(shape=[None, 784], dtype=tf.float32)\n",
    "x_image = tf.reshape(x, [-1,28,28,1])\n",
    "\n",
    "image_filters(x_image)\n",
    "image_filters(x_image)\n",
    "\n",
    "variables = tf.all_variables()\n",
    "for var in variables:\n",
    "    print(var.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Common Solution (using a Dictionary)\n",
    "\n",
    "```\n",
    "variables_dict = {\n",
    "    \"conv1_weights\": tf.Variable(tf.random_normal([5, 5, 32, 32]),\n",
    "        name=\"conv1_weights\")\n",
    "    \"conv1_biases\": tf.Variable(tf.zeros([32]), name=\"conv1_biases\")\n",
    "    #... etc. ...\n",
    "}\n",
    "\n",
    "def my_image_filter(input_images, variables_dict):\n",
    "    conv1 = tf.nn.conv2d(input_images, variables_dict[\"conv1_weights\"],\n",
    "        strides=[1, 1, 1, 1], padding='SAME')\n",
    "    relu1 = tf.nn.relu(conv1 + variables_dict[\"conv1_biases\"])\n",
    "\n",
    "    conv2 = tf.nn.conv2d(relu1, variables_dict[\"conv2_weights\"],\n",
    "        strides=[1, 1, 1, 1], padding='SAME')\n",
    "    return tf.nn.relu(conv2 + variables_dict[\"conv2_biases\"])\n",
    "\n",
    "# The 2 calls to my_image_filter() now use the same variables\n",
    "result1 = my_image_filter(image1, variables_dict)\n",
    "result2 = my_image_filter(image2, variables_dict)\n",
    "```\n",
    "## PROBLEMS (Breaks encapsulation):\n",
    "* The code that builds the graph must document the names, types, and shapes of variables to create (development issue).\n",
    "* When the code changes, the callers may have to create more, or less, or different variables (development issue).\n",
    "\n",
    "**One way to address the problem is to use classes to create a model, where the classes take care of managing the variables they need. For a lighter solution, not involving classes, TensorFlow provides a Variable Scope mechanism that allows to easily share named variables while constructing a graph.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Variable Scope:\n",
    "\n",
    "Variable scope mechanism consists of 2 primary functions:\n",
    "* `tf.get_variable(<name>, <shape>, <initializer>)`: CREATES or RETURNS a variable with a given name.\n",
    "* `tf.variable_scope(<scope_name>)`: MANAGES namespaces for names passed to `tf.get_variable`\n",
    "\n",
    "The function `tf.get_variable()` is used to get or create a variable instead of a direct call to `tf.Variable`. It uses an initializer instead of passing the value directly, as in `tf.Variable`. An initializer is a function that takes the shape and provides a tensor with that shape. Here are some initializers available in TensorFlow:\n",
    "\n",
    "* `tf.constant_initializer(value)` initializes everything to the provided value,\n",
    "* `tf.random_uniform_initializer(a, b)` initializes uniformly from [a, b],\n",
    "* `tf.random_normal_initializer(mean, stddev)` initializes from the normal distribution with the given mean and standard deviation.\n",
    "\n",
    "## Usage:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "variables = tf.all_variables()\n",
    "for var in variables:\n",
    "    print(var.name)\n",
    "\n",
    "def conv_relu(input, kernel_shape, bias_shape):\n",
    "    # Create variable named \"weights\".\n",
    "    weights = tf.get_variable(\"weights\", kernel_shape,\n",
    "        initializer=tf.random_normal_initializer())\n",
    "    # Create variable named \"biases\".\n",
    "    biases = tf.get_variable(\"biases\", bias_shape,\n",
    "        initializer=tf.constant_initializer(0.0))\n",
    "    conv = tf.nn.conv2d(input, weights,\n",
    "        strides=[1, 1, 1, 1], padding='SAME')\n",
    "    return tf.nn.relu(conv + biases)\n",
    "\n",
    "def my_image_filter(in_input):\n",
    "    with tf.variable_scope(\"conv1\"):\n",
    "        # Variables created here will be named \"conv1/weights\",\n",
    "        #                                      \"conv1/biases\".\n",
    "        relu1 = conv_relu(in_input, [5, 5, 1, 32], [32])\n",
    "    with tf.variable_scope(\"conv2\"):\n",
    "        # Variables created here will be named \"conv2/weights\",\n",
    "        #                                      \"conv2/biases\".\n",
    "        return conv_relu(relu1, [5, 5, 32, 32], [32])\n",
    "\n",
    "x = tf.placeholder(shape=[None, 784], dtype=tf.float32)\n",
    "x_image = tf.reshape(x, [-1,28,28,1])\n",
    "\n",
    "my_image_filter(x_image)\n",
    "\n",
    "variables = tf.all_variables()\n",
    "for var in variables:\n",
    "    print(var.name)\n",
    "# Now if my_image_filter is called again (we have a problem)\n",
    "# Raises ValueError( ... name already exists in scope ...)\n",
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "x = tf.placeholder(shape=[None, 784], dtype=tf.float32)\n",
    "x_image = tf.reshape(x, [-1,28,28,1])\n",
    "\n",
    "with tf.variable_scope(\"image_filters\") as scope:\n",
    "    result1 = my_image_filter(x_image)\n",
    "    scope.reuse_variables()\n",
    "    result2 = my_image_filter(x_image)\n",
    "    \n",
    "variables = tf.all_variables()\n",
    "for var in variables:\n",
    "    print(var.name) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3+"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
